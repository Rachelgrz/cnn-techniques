{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Continueing training?\n",
    "continued = False\n",
    "checkpoint_file_to_use = None #Example: 'ckpt_model_01_run01/hourly/model-9350'\n",
    "\n",
    "#If training from step 0\n",
    "#############################\n",
    "naming = 'model_09_run01'\n",
    "#############################\n",
    "variables = {\n",
    "    'modify_images': True,\n",
    "    'crop_images': True,\n",
    "    'enlarge_images': True,\n",
    "    'before_flatten_image_side_size': 6, #Need to calculate\n",
    "    'first_hidden_layer_features': 384,\n",
    "    'weights_stddev': 0.015,\n",
    "    'biases_initial': 0.1,\n",
    "    'dropout_train_keep_prob': 0.5,\n",
    "    'learning_rate_initial': 0.1,\n",
    "    'learning_rate_decay_steps': int(10000000),\n",
    "    'learning_rate_decay': 1.0,\n",
    "    'start_step_early_stopping': 150000,\n",
    "    'early_stopping_patience': 0.1,\n",
    "    'batch_size': 200,\n",
    "    'max_steps': 100000000,\n",
    "    'average_n_validation_accuracy': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create accuracy_log to pickle and directories for TensorBoard and checkpoint\n",
    "accuracy_log = prep_accuracy_log(continued, checkpoint_file_to_use, naming, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get data and labels\n",
    "(train_data, validation_data, test_data, train_labels, validation_labels, test_labels) = get_data_and_labels(accuracy_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a model\n",
    "crop_images = accuracy_log['variables']['crop_images']\n",
    "modify_images = accuracy_log['variables']['modify_images']\n",
    "weights_stddev = accuracy_log['variables']['weights_stddev']\n",
    "biases_initial = accuracy_log['variables']['biases_initial']\n",
    "input_image_side_size = accuracy_log['variables']['input_image_side_size']\n",
    "crop_to_side_size = accuracy_log['variables']['crop_to_side_size']\n",
    "before_flatten_image_side_size = accuracy_log['variables']['before_flatten_image_side_size']\n",
    "first_hidden_layer_features = accuracy_log['variables']['first_hidden_layer_features']\n",
    "learning_rate_initial = accuracy_log['variables']['learning_rate_initial']\n",
    "learning_rate_decay_steps = accuracy_log['variables']['learning_rate_decay_steps']\n",
    "learning_rate_decay = accuracy_log['variables']['learning_rate_decay']\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    data = tf.placeholder(tf.float32, [None, input_image_side_size, input_image_side_size, 3])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    if crop_images & modify_images:\n",
    "        data_v2 = tf.cond(is_training, lambda: random_modify(data, crop_to_side_size), lambda: crop_center(data, crop_to_side_size))\n",
    "    elif (not crop_images) & modify_images:\n",
    "        data_v2 = tf.cond(is_training, lambda: random_modify_no_crop(data), lambda: tf.identity(data))\n",
    "    elif crop_images & (not modify_images):\n",
    "        data_v2 = tf.cond(is_training, lambda: random_crop(data, crop_to_side_size), lambda: crop_center(data, crop_to_side_size))\n",
    "    else:\n",
    "        data_v2 = tf.identity(data)\n",
    "\n",
    "    data_v2 = batch_normalize(data_v2, is_training=is_training, global_step=global_step, scope='bn_data_v2')\n",
    "    W_conv1 = weight_variable([3, 3, 3, 64], weights_stddev, 'W_conv1')\n",
    "    b_conv1 = bias_variable(biases_initial, [64], 'b_conv1')\n",
    "    conv1 = conv2d(data_v2, W_conv1) + b_conv1\n",
    "    conv1_relu = tf.nn.relu(conv1)\n",
    "    pool1 = max_pool_3x3_stride2(conv1_relu)\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "    norm1 = batch_normalize(norm1, is_training=is_training, global_step=global_step, scope='bn_norm1')\n",
    "    W_conv2 = weight_variable([3, 3, 64, 64], weights_stddev, 'W_conv2')\n",
    "    b_conv2 = bias_variable(biases_initial, [64], 'b_conv2')\n",
    "    conv2 = conv2d(norm1, W_conv2) + b_conv2\n",
    "    conv2_relu = tf.nn.relu(conv2)\n",
    "    pool2 = max_pool_3x3_stride2(conv2_relu)\n",
    "    norm2 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "    norm2 = batch_normalize(norm2, is_training=is_training, global_step=global_step, scope='bn_norm2')\n",
    "    W_conv3 = weight_variable([3, 3, 64, 128], weights_stddev, 'W_conv3')\n",
    "    b_conv3 = bias_variable(biases_initial, [128], 'b_conv3')\n",
    "    conv3 = conv2d(norm2, W_conv3) + b_conv3\n",
    "    conv3_relu = tf.nn.relu(conv3)\n",
    "    pool3 = max_pool_3x3_stride2(conv3_relu)\n",
    "    norm3 = tf.nn.lrn(pool3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "    norm3 = batch_normalize(norm3, is_training=is_training, global_step=global_step, scope='bn_norm3')\n",
    "    W_conv4 = weight_variable([3, 3, 128, 128], weights_stddev, 'W_conv4')\n",
    "    b_conv4 = bias_variable(biases_initial, [128], 'b_conv4')\n",
    "    conv4 = conv2d(norm3, W_conv4) + b_conv4\n",
    "    conv4_relu = tf.nn.relu(conv4)\n",
    "    pool4 = max_pool_3x3_stride1(conv4_relu)\n",
    "    norm4 = tf.nn.lrn(pool4, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "    norm4_flat = tf.reshape(norm4, [-1, before_flatten_image_side_size*before_flatten_image_side_size*128])\n",
    "\n",
    "    norm4_flat = batch_normalize(norm4_flat, is_training=is_training, global_step=global_step, scope='bn_norm4_flat')\n",
    "    norm4_flat_drop = tf.nn.dropout(norm4_flat, keep_prob)\n",
    "    W_fc1 = weight_variable([before_flatten_image_side_size*before_flatten_image_side_size*128, first_hidden_layer_features], weights_stddev, 'W_fc1')\n",
    "    b_fc1 = bias_variable(biases_initial, [first_hidden_layer_features], 'b_fc1')\n",
    "    fc1 = tf.matmul(norm4_flat_drop, W_fc1) + b_fc1\n",
    "    fc1_relu = tf.nn.relu(fc1)\n",
    "\n",
    "    fc1_relu = batch_normalize(fc1_relu, is_training=is_training, global_step=global_step, scope='bn_fc1_relu')    \n",
    "    fc1_relu_drop = tf.nn.dropout(fc1_relu, keep_prob)\n",
    "    W_fc2 = weight_variable([first_hidden_layer_features, int(first_hidden_layer_features/2)], weights_stddev, 'W_fc2')\n",
    "    b_fc2 = bias_variable(biases_initial, [first_hidden_layer_features/2], 'b_fc2')\n",
    "    fc2 = tf.matmul(fc1_relu_drop, W_fc2) + b_fc2\n",
    "    fc2_relu = tf.nn.relu(fc2)\n",
    "\n",
    "    fc2_relu = batch_normalize(fc2_relu, is_training=is_training, global_step=global_step, scope='bn_fc2_relu')\n",
    "    fc2_relu_drop = tf.nn.dropout(fc2_relu, keep_prob)\n",
    "    W_fc3 = weight_variable([int(first_hidden_layer_features/2), 10], weights_stddev, 'W_fc3')\n",
    "    b_fc3 = bias_variable(biases_initial, [10], 'b_fc3')\n",
    "    logits = tf.matmul(fc2_relu_drop, W_fc3) + b_fc3\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate_initial, global_step, learning_rate_decay_steps, \n",
    "                                               learning_rate_decay, staircase=True)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    tf.summary.scalar('training_accuracy', accuracy)\n",
    "\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    summarizer = tf.summary.merge_all()\n",
    "    \n",
    "graph_variables = {'graph': graph, \n",
    "                   'optimizer': optimizer, \n",
    "                   'summarizer': summarizer, \n",
    "                   'data': data, \n",
    "                   'labels': labels, \n",
    "                   'keep_prob': keep_prob, \n",
    "                   'is_training': is_training, \n",
    "                   'correct_prediction': correct_prediction,\n",
    "                   'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "train(continued, graph_variables, accuracy_log, train_data, train_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< test accuracy 0.88160 >>\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy\n",
    "test_accuracy(graph_variables, accuracy_log, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
